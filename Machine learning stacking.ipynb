{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import machine_learning_helper as machine_learning_helper\n",
    "import metrics_helper as metrics_helper\n",
    "import sklearn.neighbors, sklearn.linear_model, sklearn.ensemble, sklearn.naive_bayes\n",
    "from sklearn.model_selection import KFold, train_test_split, ShuffleSplit\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import scipy as sp\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_users = pd.read_csv(\"cleaned_train_user.csv\")\n",
    "df_test_users = pd.read_csv(\"cleaned_test_user.csv\")\n",
    "df_time_mean_user_id = pd.read_csv(\"time_mean_user_id.csv\")\n",
    "df_time_total_user_id = pd.read_csv(\"time_total_user_id.csv\")\n",
    "df_total_action_user_id = pd.read_csv(\"total_action_user_id.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct sessions data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has dimension: (213451, 16)\n",
      "X_test has dimension: (62096, 15)\n"
     ]
    }
   ],
   "source": [
    "df_total_action_user_id.columns = ['id','action']\n",
    "df_sessions = pd.merge(df_time_mean_user_id, df_time_total_user_id, on='id', how='outer')\n",
    "df_sessions = pd.merge(df_sessions, df_total_action_user_id, on='id', how='outer')\n",
    "df_sessions.columns = ['id','time_mean_user','time_total_user','action']\n",
    "df_sessions.head()\n",
    "\n",
    "print(\"X_train has dimension:\",df_train_users.shape)\n",
    "print(\"X_test has dimension:\",df_test_users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From data frame to matrix : Construct y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want now for the training is 2 matrices X_train (matrix of relevant features) and y_train (booking dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_labels, label_enc = machine_learning_helper.buildTargetMat(df_train_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From data frame to matrix : Construct X_train & X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering.\n",
    "Add 3 features : \n",
    "- time_mean_user\n",
    "- time_total_user\n",
    "- total_action_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_len = df_train_users.shape[0]\n",
    "df_train = df_train_users.drop(['country_destination'],axis=1)\n",
    "df_all = pd.concat((df_train_users, df_test_users), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_sessions, on='id', how='left', left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = machine_learning_helper.buildFeatsMat(df_train_users, df_test_users, df_sessions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "5 folds cross validation, using ndcg as scoring metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train = X_train[100000:100100]\n",
    "#y_labels = y_labels[100000:100100]\n",
    "#X_test = X_test[10000:60000]\n",
    "\n",
    "# Split train dataset into 5 folds \n",
    "cv = model_selection.KFold(n_splits=5, random_state=None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 : eXtreme Gradient Boosting XCGB\n",
    "\n",
    "5 folds cross validation, using ndcg as scoring metric.\n",
    "\n",
    "Grid Search to find best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rates = [ 0.1,0.2,0.3,0.4]\n",
    "max_depth = [6, 8, 10, 12, 14, 16, 20]\n",
    "n_estimators = [10,30,50,75,100]\n",
    "\n",
    "rf_score_rates = []\n",
    "rf_score_depth = []\n",
    "rf_score_estimators = []\n",
    "rf_param_rates = []\n",
    "rf_param_depth = []\n",
    "rf_param_estimators = []\n",
    "\n",
    "#Loop for 1st hyperparameter n_estimators\n",
    "for n_estimators_idx, n_estimators_value in enumerate(n_estimators):\n",
    "    \n",
    "    print('n_estimators_idx: ',n_estimators_idx+1,'/',len(n_estimators),', value: ', n_estimators_value)\n",
    "\n",
    "    # XCGB\n",
    "    model = XGBClassifier(max_depth=10, learning_rate=0.2, n_estimators=n_estimators_value,objective='multi:softprob',\n",
    "                      subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "\n",
    "    #Scores\n",
    "    scores = model_selection.cross_val_score(model, X_train_sparse, y_labels, cv=cv, verbose = 10, n_jobs = 12, scoring=metrics_helper.ndcg_scorer)\n",
    "    rf_score_estimators.append(scores.mean())\n",
    "    rf_param_estimators.append(n_estimators_value)\n",
    "    print('Mean NDCG for this n_estimators = ', scores.mean())\n",
    "\n",
    "# best number of estimators from above\n",
    "print() \n",
    "print('best NDCG:')\n",
    "print(np.max(rf_score_estimators))\n",
    "print('best parameter num_estimators:')\n",
    "idx_best = np.argmax(rf_score_estimators)\n",
    "best_num_estimators_XCG = rf_param_estimators[idx_best]\n",
    "print(best_num_estimators_XCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loop for  hyperparameter max_depth\n",
    "for max_depth_idx, max_depth_value in enumerate(max_depth):\n",
    "    \n",
    "    print('max_depth_idx: ',max_depth_idx+1,'/',len(max_depth),', value: ', max_depth_value)\n",
    "\n",
    "    # XCGB\n",
    "    model = XGBClassifier(max_depth=max_depth_value, learning_rate=0.2, n_estimators=10,objective='multi:softprob',\n",
    "                      subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "\n",
    "    #Scores\n",
    "    scores = model_selection.cross_val_score(model, X_train_sparse, y_labels, cv=cv, verbose = 10, n_jobs = 12, scoring=metrics_helper.ndcg_scorer)\n",
    "    rf_score_depth.append(scores.mean())\n",
    "    rf_param_depth.append(max_depth_value)\n",
    "    print('Mean NDCG for this max_depth = ', scores.mean())\n",
    "\n",
    "# best number of estimators from above\n",
    "print() \n",
    "print('best NDCG:')\n",
    "print(np.max(rf_score_depth))\n",
    "print('best parameter max_depth:')\n",
    "idx_best = np.argmax(rf_score_depth)\n",
    "best_num_depth_XCG = rf_param_depth[idx_best]\n",
    "print(best_num_depth_XCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loop for  hyperparameter learning rate\n",
    "for learning_rates_idx, learning_rates_value in enumerate(learning_rates):\n",
    "    \n",
    "    print('learning_rates_idx: ',learning_rates_idx+1,'/',len(learning_rates),', value: ', learning_rates_value)\n",
    "\n",
    "    # XGB\n",
    "    XGB_model = xgb.XGBClassifier(max_depth=6, learning_rate=learning_rates_value, n_estimators=10,objective='multi:softprob',\n",
    "                      subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "\n",
    "    #Scores\n",
    "    scores = model_selection.cross_val_score(model, X_train_sparse, y_labels, cv=cv, verbose = 10, n_jobs = 12, scoring=metrics_helper.ndcg_scorer)\n",
    "    rf_score_rates.append(scores.mean())\n",
    "    rf_param_rates.append(learning_rates_value)\n",
    "    print('Mean NDCG for this learning rate = ', scores.mean())\n",
    "\n",
    "# best number of trees from above\n",
    "print() \n",
    "print('best NDCG:')\n",
    "print(np.max(rf_score_rates))\n",
    "print('best parameter learning rates:')\n",
    "idx_best = np.argmax(rf_score_rates)\n",
    "best_learning_rate_XCG = rf_param_rates[idx_best]\n",
    "print(best_learning_rate_XCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Countries and convert to CSV for submision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XGB_model = XGBClassifier(max_depth=best_num_depth_XCG, learning_rate=best_learning_rate_XCG, n_estimators=best_num_estimators_XCG,objective='multi:softprob',\n",
    "                      subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "XGB_model.fit(X_train,y_labels)\n",
    "y_pred2 = XGB_model.predict_proba(X_test)  \n",
    "id_test = df_test_users['id']\n",
    "cts2,idsubmission2 = machine_learning_helper.get5likelycountries(y_pred2, id_test)\n",
    "\n",
    "ctsSubmission2 = label_enc.inverse_transform(cts2)\n",
    "\n",
    "\n",
    "df_submission2 = pd.DataFrame(np.column_stack((idsubmission2, ctsSubmission2)), columns=['id', 'country'])\n",
    "df_submission2.to_csv('submission_country_dest_XGB.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 : Stacking \n",
    "\n",
    "As seen previously, the classes in this dataset are imbalanced. Indeed, half of the users didn't book. We are going to use that information.\n",
    "\n",
    "This model is composed of 2 layers. In a first layer, a logistic regression determines if a user is going to book or not. This binary classification model is trained on the training set. The prediction on the test set by this model is added to a second layer, as a meta feature.\n",
    "\n",
    "The second layer is a XGBoost algorithm. It is trained on the training set, with a column 'meta_layer_1' added, that comes from the first layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1 : Logistic regression\n",
    "\n",
    "This logistic regressionw will determine if a user booked or not. It is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build 1st layer training matrix, text matrix, target vector\n",
    "y_labels_binary, X_train_layer1, X_test_layer1 = machine_learning_helper.buildFeatsMatBinary(df_train_users, df_test_users, df_sessions)\n",
    "y_labels_binary = y_labels_binary.astype(np.int8)\n",
    "\n",
    "# Build 1st layer model\n",
    "# Cross validation with parameter C\n",
    "\n",
    "C = [0.1, 1.0, 10]\n",
    "logistic_score_C = []\n",
    "logistic_param_C = []\n",
    "\n",
    "#Loop for hyperparameter \n",
    "for C_idx, C_value in enumerate(C):\n",
    "    \n",
    "    print('C_idx: ',C_idx+1,'/',len(C),', value: ', C_value)\n",
    "\n",
    "    # SVM\n",
    "    model = linear_model.LogisticRegression(c = C_value)\n",
    "\n",
    "    #Scores\n",
    "    scores = model_selection.cross_val_score(model, X_train_layer1, y_labels_binary, cv=cv, verbose = 10, n_jobs = 12)\n",
    "    logistic_score_C.append(scores.mean())\n",
    "    logistic_param_C.append(C_value)\n",
    "    print('Mean NDCG for this C = ', scores.mean())\n",
    "\n",
    "# best C from above\n",
    "print() \n",
    "print('best accuracy:')\n",
    "print(np.max(logistic_score_C))\n",
    "print('best parameter C:')\n",
    "idx_best = np.argmax(logistic_score_C)\n",
    "best_C_logistic = logistic_param_C[idx_best]\n",
    "print(best_C_logistic)\n",
    "\n",
    "# Build model with best parameter from cross validation\n",
    "logreg_layer1 = linear_model.LogisticRegression(c = best_C_logistic)\n",
    "logreg_layer1.fit(X_train_layer1, y_labels_binary)\n",
    "\n",
    "# 1st layer model prediction\n",
    "prediction_layer_1 = logreg_layer1.predict(X_test_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 : XGBoost\n",
    "\n",
    "Using the previous result as a meta_feature, this model will determine the 5 most likely countries in which a user will travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_num_depth_XCG = 7\n",
    "best_learning_rate_XCG = 0.1\n",
    "best_num_estimators_XCG = 75\n",
    "best_gamma_XCG = 1\n",
    "\n",
    "# Build 2nd layer training matrix, text matrix, target vector\n",
    "X_train_layer2 = X_train_layer1\n",
    "X_train_layer2['meta_layer_1'] = pd.Series(y_labels_binary).astype(np.int8)\n",
    "X_test_layer2 = X_test_layer1\n",
    "X_test_layer2['meta_layer_1'] = pd.Series(prediction_layer_1).astype(np.int8)\n",
    "\n",
    "# Build 2nd layer model\n",
    "best_learning_rate_XCG = 0.1\n",
    "best_num_depth_XCG = 7\n",
    "best_gamma_XCG = 1\n",
    "best_num_estimators_XCG = 75\n",
    "\n",
    "XGB_model = XGBClassifier(max_depth=best_num_depth_XCG, learning_rate=best_learning_rate_XCG, n_estimators=best_num_estimators_XCG,objective='multi:softprob',\n",
    "                      subsample=0.5, colsample_bytree=0.5, gamma = best_gamma_XCG)\n",
    "XGB_model.fit(X_train_layer2,y_labels)\n",
    "y_pred2 = XGB_model.predict_proba(X_test_layer2)  \n",
    "id_test = df_test_users['id']\n",
    "cts2,idsubmission2 = machine_learning_helper.get5likelycountries(y_pred2, id_test)\n",
    "\n",
    "ctsSubmission2 = label_enc.inverse_transform(cts2)\n",
    "\n",
    "\n",
    "df_submission2 = pd.DataFrame(np.column_stack((idsubmission2, ctsSubmission2)), columns=['id', 'country'])\n",
    "df_submission2.to_csv('submission_country_dest_stacking.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
